# -*- coding: utf-8 -*-
# -----------------------------------------------------------------------------
# æ­¥éª¤ 1: å¯¼å…¥æ‰€æœ‰å¿…è¦çš„åº“
# -----------------------------------------------------------------------------
import os
import json
import logging
import time
import datetime
import pytz # ç”¨äºå¤„ç†æ—¶åŒº
import streamlit as st
from openai import OpenAI
from dotenv import load_dotenv
import chromadb
from chromadb.utils import embedding_functions
import hashlib
import re

# -----------------------------------------------------------------------------
# æ­¥éª¤ 2: åç«¯é€»è¾‘ä»£ç 
# -----------------------------------------------------------------------------

# --- æ—¥å¿—è®°å½•é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- æ ¸å¿ƒé…ç½®ç±» ---
class Config:
    """é›†ä¸­ç®¡ç†æ‰€æœ‰é…ç½®"""
    load_dotenv()
    ZHIPU_BASE_URL = "https://open.bigmodel.cn/api/paas/v4/"
    LLM_MODEL = "glm-4-flash"
    THINKING_MODEL = "glm-z1-flash"
    EMBEDDING_MODEL = "<å¡«å…¥å‘é‡æ¨¡å‹åœ°å€>"
    DB_PATH = "<æ•°æ®å­˜å‚¨è·¯å¾„>" 
    CHAT_COLLECTION_NAME = "user_chat_history"
    FACT_MEMORY_COLLECTION_NAME = "user_fact_memory"
    EVENT_MEMORY_COLLECTION_NAME = "user_event_memory"
    RAG_COLLECTION_NAME = "user_rag_documents"
    TIMEZONE = "Asia/Shanghai"
    SYSTEM_PROMPT_TEMPLATE = """
    ä½ æ˜¯ä¸€ä½ä¸ºç”¨æˆ· {user_id} æœåŠ¡çš„ã€å……æ»¡æ¸©åº¦ä¸æ”¯æŒçš„ç§äººä¼™ä¼´ã€‚

    # ä½ çš„æ ¸å¿ƒè§’è‰²:
    - **æˆä¸ºä¼™ä¼´, è€Œéè€å¸ˆ**: ä½ çš„é¦–è¦ç›®æ ‡æ˜¯æˆä¸ºä¸€ä¸ªä¹äºåŠ©äººã€æœ‰åŒç†å¿ƒçš„ä¼™ä¼´ã€‚ä½ çš„è§’è‰²ä¸æ˜¯çº æ­£ç”¨æˆ·çš„é”™è¯¯ï¼Œè€Œæ˜¯é™ªä¼´å’Œæ”¯æŒä»–ä»¬æ¢³ç†æ€ç»ªã€è§„åˆ’ç”Ÿæ´»ã€‚
    - **ç§¯æã€é¼“åŠ±ã€æœ‰æ¸©åº¦**: å§‹ç»ˆä¿æŒç§¯æå’Œé¼“åŠ±çš„æ€åº¦ã€‚åœ¨å›åº”æ—¶ï¼Œå¤šä¸€äº›å…³å¿ƒå’Œç†è§£ï¼Œå°‘ä¸€äº›ç”Ÿç¡¬çš„æŒ‡ä»¤å’Œè¯´æ•™ã€‚
    - **ä¸ªæ€§åŒ–äº’åŠ¨**: åƒä¸€ä¸ªçœŸæ­£çš„æœ‹å‹ä¸€æ ·ï¼Œè‡ªç„¶åœ°è¿ç”¨æˆ‘ä¸ºä½ æä¾›çš„æ‰€æœ‰ä¿¡æ¯ï¼ˆç”¨æˆ·çš„ä¸ªäººäº‹å®ã€äº‹ä»¶è®°å¿†ã€çŸ¥è¯†åº“ç­‰ï¼‰ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£ä¸Šä¸‹æ–‡ï¼Œå¹¶ç»™å‡ºè´´å¿ƒã€ä¸ªæ€§åŒ–çš„å›åº”ã€‚

    # å‚è€ƒä¿¡æ¯ (æˆ‘ä¼šä¸ºä½ æä¾›):
    
    ## 1. æ—¶é—´ä¸æ—¥æœŸ
    - **å½“å‰ç²¾ç¡®æ—¶é—´**: {current_time}
    - **æœªæ¥ä¸€å‘¨æ—¥æœŸå‚è€ƒ**: 
      ä¸ºäº†å¸®åŠ©ä½ å‡†ç¡®è®¡ç®—æ—¥æœŸï¼Œè¿™é‡Œæ˜¯æ¥ä¸‹æ¥ä¸€å‘¨çš„æ—¥æœŸä¿¡æ¯ã€‚**è¯·ä¼˜å…ˆä½¿ç”¨æ­¤ä¿¡æ¯å›ç­”ä¸æ—¥æœŸç›¸å…³çš„é—®é¢˜ã€‚**
      {date_reference}

    ## 2. å…³äºç”¨æˆ·çš„è®°å¿†
    - **é•¿æœŸäº‹å®**: {long_term_memory}
    - **æœªæ¥è®¡åˆ’**: {future_events}
    - **è¿‘æœŸäº‹ä»¶**: {past_events}

    # ä½ çš„å›åº”æ–¹å¼:
    - æ·±å…¥ç†è§£ç”¨æˆ·çš„æ„å›¾ï¼Œç»“åˆæ‰€æœ‰å·²çŸ¥ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªè‡ªç„¶ã€æµç•…ã€ä¸”å……æ»¡ä¼™ä¼´æ„Ÿçš„å›ç­”ã€‚
    - å¦‚æœçŸ¥è¯†åº“ä¿¡æ¯ç›¸å…³ï¼Œè¯·ä»¥ä¸€ç§å»ºè®®æˆ–â€œæˆ‘å‘ç°è¿™ä¸ªå¯èƒ½æœ‰ç”¨â€çš„å£å»æ¥åˆ†äº«ï¼Œè€Œä¸æ˜¯ä½œä¸ºç»å¯¹äº‹å®ã€‚
    """

def process_response_for_display(content: str) -> str:
    """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç§»é™¤<think>...</think>æ ‡ç­¾åŠå…¶ä¸­çš„æ‰€æœ‰å†…å®¹ã€‚"""
    return re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()

def simple_text_splitter(text: str, max_chunk_size: int = 500) -> list[str]:
    """ä¸€ä¸ªç®€å•çš„æ–‡æœ¬åˆ†å‰²å™¨ï¼ŒæŒ‰å¥å­åˆ†å‰²"""
    sentences = text.replace("\n", " ").replace("\r", " ").split('ã€‚')
    chunks, current_chunk = [], ""
    for sentence in sentences:
        if not sentence.strip(): continue
        sentence += "ã€‚"
        if len(current_chunk) + len(sentence) <= max_chunk_size:
            current_chunk += sentence
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sentence
    if current_chunk: chunks.append(current_chunk.strip())
    return chunks

class ChatHistoryDB:
    def __init__(self, config: Config):
        self.config = config
        self.tz = pytz.timezone(config.TIMEZONE)
        try:
            os.makedirs(config.DB_PATH, exist_ok=True)
            self.db_client = chromadb.PersistentClient(path=self.config.DB_PATH)
            self.embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(
                model_name=self.config.EMBEDDING_MODEL
            )
            self.chat_collection = self.db_client.get_or_create_collection(name=self.config.CHAT_COLLECTION_NAME, embedding_function=self.embedding_func)
            self.fact_memory_collection = self.db_client.get_or_create_collection(name=self.config.FACT_MEMORY_COLLECTION_NAME, embedding_function=self.embedding_func)
            self.event_memory_collection = self.db_client.get_or_create_collection(name=self.config.EVENT_MEMORY_COLLECTION_NAME, embedding_function=self.embedding_func)
            self.rag_collection = self.db_client.get_or_create_collection(name=self.config.RAG_COLLECTION_NAME, embedding_function=self.embedding_func)
            logging.info(f"æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ: {config.DB_PATH}")
        except Exception as e:
            logging.error(f"åˆå§‹åŒ– ChromaDB å¤±è´¥: {e}")
            st.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def add_document_to_rag(self, user_id: str, file_name: str, file_content: str, progress_callback=None):
        if progress_callback: progress_callback(0, "æ­¥éª¤ 1/2: æ­£åœ¨åˆ†å‰²æ–‡ä»¶...")
        chunks = simple_text_splitter(file_content)
        if not chunks:
            if progress_callback: progress_callback(100, "æ–‡ä»¶å†…å®¹ä¸ºç©ºï¼Œå·²è·³è¿‡ã€‚")
            return

        total_chunks = len(chunks)
        logging.info(f"æ–‡ä»¶ '{file_name}' è¢«åˆ†å‰²æˆ {total_chunks} ä¸ªç‰‡æ®µã€‚")
        
        if progress_callback: progress_callback(5, f"æ­¥éª¤ 2/2: å‡†å¤‡è®¡ç®—å‘é‡... (å…± {total_chunks} å—)")
        
        batch_size = 32
        for i in range(0, total_chunks, batch_size):
            batch_chunks = chunks[i:i + batch_size]
            batch_ids = [f"rag_{user_id}_{file_name}_{time.time()}_{i+j}" for j in range(len(batch_chunks))]
            batch_metadatas = [{"user_id": user_id, "source": file_name} for _ in batch_chunks]
            
            self.rag_collection.add(ids=batch_ids, documents=batch_chunks, metadatas=batch_metadatas)
            
            if progress_callback:
                processed_count = i + len(batch_chunks)
                percentage = 5 + int((processed_count / total_chunks) * 90)
                status_text = f"æ­¥éª¤ 2/2: æ­£åœ¨è®¡ç®—å‘é‡... ({processed_count}/{total_chunks})"
                progress_callback(min(percentage, 95), status_text)
        
        if progress_callback: progress_callback(100, "çŸ¥è¯†åº“å­¦ä¹ å®Œæˆï¼")
        logging.info(f"æ–‡ä»¶ '{file_name}' å·²æˆåŠŸæ·»åŠ è‡³çŸ¥è¯†åº“ã€‚")

    def save_message(self, user_id: str, message: dict):
        if not all(k in message for k in ['role', 'content']) or not message.get('content'): return
        timestamp = time.time()
        doc_id = f"msg_{user_id}_{timestamp}"
        self.chat_collection.add(ids=[doc_id], documents=[message['content']], metadatas=[{"user_id": user_id, "role": message['role'], "timestamp": timestamp}])

    def load_history_by_user(self, user_id: str) -> list:
        if not user_id: return []
        results = self.chat_collection.get(where={"user_id": user_id})
        if not results['ids']: return []
        packaged_messages = sorted([{"doc": results['documents'][i], "meta": results['metadatas'][i]} for i in range(len(results['ids']))], key=lambda m: m['meta']['timestamp'])
        return [{"role": msg['meta']['role'], "content": msg['doc']} for msg in packaged_messages]

    def clear_user_chat_history(self, user_id: str):
        if self.chat_collection.get(where={"user_id": user_id})['ids']:
            self.chat_collection.delete(where={"user_id": user_id})
            logging.info(f"å·²æ¸…ç©ºç”¨æˆ· {user_id} çš„å¯¹è¯å†å²ã€‚")

    def clear_old_temporal_fact_memory(self, user_id: str):
        logging.info(f"å¼€å§‹ä¸ºç”¨æˆ· {user_id} æ¸…ç†æ—§çš„ã€ä¸´æ—¶ã€‘äº‹å®è®°å¿†...")
        
        today_start = datetime.datetime.now(self.tz).replace(hour=0, minute=0, second=0, microsecond=0)
        today_start_timestamp = today_start.timestamp()

        results = self.fact_memory_collection.get(where={"user_id": user_id})
        if not results['ids']:
            logging.info(f"ç”¨æˆ· {user_id} æ²¡æœ‰äº‹å®è®°å¿†å¯æ¸…ç†ã€‚")
            return

        ids_to_delete = [
            results['ids'][i] 
            for i, meta in enumerate(results['metadatas']) 
            if meta.get('is_permanent') is not True and meta.get('timestamp') and meta['timestamp'] < today_start_timestamp
        ]

        if ids_to_delete:
            logging.info(f"ä¸ºç”¨æˆ· {user_id} æ‰¾åˆ° {len(ids_to_delete)} æ¡æ—§çš„ä¸´æ—¶äº‹å®è®°å¿†ï¼Œå‡†å¤‡åˆ é™¤...")
            self.fact_memory_collection.delete(ids=ids_to_delete)
            logging.info(f"å·²æˆåŠŸä¸ºç”¨æˆ· {user_id} æ¸…ç†äº† {len(ids_to_delete)} æ¡æ—§çš„ä¸´æ—¶äº‹å®è®°å¿†ã€‚")
        else:
            logging.info(f"ç”¨æˆ· {user_id} æ²¡æœ‰ä»Šå¤©ä¹‹å‰çš„æ—§ä¸´æ—¶äº‹å®è®°å¿†å¯æ¸…ç†ã€‚")

    def clear_expired_events(self, user_id: str):
        logging.info(f"å¼€å§‹ä¸ºç”¨æˆ· {user_id} æ¸…ç†è¿‡æœŸçš„äº‹ä»¶...")
        
        now = datetime.datetime.now(self.tz)
        
        results = self.event_memory_collection.get(where={"user_id": user_id})
        if not results['ids']:
            logging.info(f"ç”¨æˆ· {user_id} æ²¡æœ‰äº‹ä»¶è®°å¿†å¯æ¸…ç†ã€‚")
            return

        ids_to_delete = []
        for i, meta in enumerate(results['metadatas']):
            event_time_iso = meta.get('event_time_iso')
            if not event_time_iso:
                continue
            
            try:
                parsed_dt = datetime.datetime.fromisoformat(event_time_iso.replace('Z', '+00:00'))
                if parsed_dt.tzinfo is None:
                    event_dt = self.tz.localize(parsed_dt)
                else:
                    event_dt = parsed_dt.astimezone(self.tz)
                
                if event_dt < now:
                    ids_to_delete.append(results['ids'][i])
            except (ValueError, TypeError):
                logging.warning(f"æ— æ³•è§£æç”¨æˆ· {user_id} çš„äº‹ä»¶æ—¶é—´æˆ³: {event_time_iso}ï¼Œè·³è¿‡æ¸…ç†ã€‚")
                continue

        if ids_to_delete:
            logging.info(f"ä¸ºç”¨æˆ· {user_id} æ‰¾åˆ° {len(ids_to_delete)} æ¡è¿‡æœŸäº‹ä»¶ï¼Œå‡†å¤‡åˆ é™¤...")
            self.event_memory_collection.delete(ids=ids_to_delete)
            logging.info(f"å·²æˆåŠŸä¸ºç”¨æˆ· {user_id} æ¸…ç†äº† {len(ids_to_delete)} æ¡è¿‡æœŸäº‹ä»¶ã€‚")
        else:
            logging.info(f"ç”¨æˆ· {user_id} æ²¡æœ‰è¿‡æœŸçš„äº‹ä»¶å¯æ¸…ç†ã€‚")
            
    def save_structured_memory(self, user_id: str, memory_data: dict):
        """ä¿å­˜ç»“æ„åŒ–çš„è®°å¿†ï¼Œå¤„ç†äº‹å®å’Œæ™ºèƒ½äº‹ä»¶æ“ä½œ"""

        def _save_facts(facts: dict, is_permanent: bool):
            if not (facts and isinstance(facts, dict)): return
            
            fact_type_str = "æ°¸ä¹…" if is_permanent else "ä¸´æ—¶"
            logging.info(f"æ­£åœ¨ä¸ºç”¨æˆ· {user_id} ä¿å­˜æˆ–æ›´æ–° {len(facts)} æ¡{fact_type_str}äº‹å®è®°å¿†...")
            
            items_to_save = []
            for key, value in facts.items():
                if isinstance(value, dict):
                    for sub_key, sub_value in value.items():
                        items_to_save.append((f"{key}_{sub_key}", sub_value))
                else:
                    items_to_save.append((key, value))
            
            for key, value in items_to_save:
                doc_id = f"fact_{user_id}_{key}"
                self.fact_memory_collection.upsert(
                    ids=[doc_id],
                    documents=[f"ç”¨æˆ·çš„ä¸ªäººä¿¡æ¯ï¼š{key} æ˜¯ {value}ã€‚"],
                    metadatas=[{"user_id": user_id, "key": key, "timestamp": time.time(), "is_permanent": is_permanent}]
                )

        permanent_facts = memory_data.get('permanent_facts', {})
        _save_facts(permanent_facts, is_permanent=True)

        temporal_facts = memory_data.get('temporal_facts', {})
        if not temporal_facts and 'static_facts' in memory_data:
             temporal_facts = memory_data.get('static_facts', {})
        _save_facts(temporal_facts, is_permanent=False)

        event_actions = memory_data.get('event_actions', [])
        if event_actions and isinstance(event_actions, list):
            logging.info(f"æ­£åœ¨ä¸ºç”¨æˆ· {user_id} æ‰§è¡Œ {len(event_actions)} æ¡äº‹ä»¶æ“ä½œ...")
            
            ids_to_upsert, docs_to_upsert, metas_to_upsert = [], [], []
            ids_to_delete = []

            for action in event_actions:
                action_type = action.get('type')
                event_id = action.get('event_id')
                if not (action_type and event_id): continue

                db_id = f"event_{user_id}_{event_id}"

                if action_type == 'upsert':
                    data = action.get('data', {})
                    description = data.get('description')
                    event_time_iso = data.get('event_time_iso')
                    if description and event_time_iso:
                        ids_to_upsert.append(db_id)
                        docs_to_upsert.append(description)
                        metas_to_upsert.append({
                            "user_id": user_id,
                            "event_id": event_id,
                            "event_time_iso": event_time_iso,
                            "saved_at": time.time()
                        })
                elif action_type == 'delete':
                    ids_to_delete.append(db_id)

            if ids_to_upsert:
                self.event_memory_collection.upsert(ids=ids_to_upsert, documents=docs_to_upsert, metadatas=metas_to_upsert)
                logging.info(f"å·²ä¸ºç”¨æˆ· {user_id} æ–°å¢/æ›´æ–° {len(ids_to_upsert)} æ¡äº‹ä»¶ã€‚")
            
            if ids_to_delete:
                self.event_memory_collection.delete(ids=ids_to_delete)
                logging.info(f"å·²ä¸ºç”¨æˆ· {user_id} åˆ é™¤ {len(ids_to_delete)} æ¡äº‹ä»¶ã€‚")

    def load_fact_memory(self, user_id: str, top_k: int = 20) -> str:
        results = self.fact_memory_collection.get(where={"user_id": user_id}, limit=top_k)
        return "\n".join(f"- {doc}" for doc in results.get('documents', [])) or "æš‚æ— "
    
    def load_event_memory(self, user_id: str, query: str = None) -> tuple[str, str]:
        if not user_id: return "æš‚æ— ", "æš‚æ— "
        
        all_events_result = self.event_memory_collection.get(where={"user_id": user_id})
        if not all_events_result['ids']: return "æš‚æ— ", "æš‚æ— "

        now = datetime.datetime.now(self.tz)
        future_events, past_events = [], []

        for i, meta in enumerate(all_events_result['metadatas']):
            event_time_iso = meta.get('event_time_iso')
            if not event_time_iso: continue
            
            try:
                parsed_dt = datetime.datetime.fromisoformat(event_time_iso.replace('Z', '+00:00'))
                if parsed_dt.tzinfo is None:
                    event_dt = self.tz.localize(parsed_dt)
                else:
                    event_dt = parsed_dt
            except (ValueError, TypeError):
                continue

            event_record = (event_dt, all_events_result['documents'][i])
            if event_dt > now:
                future_events.append(event_record)
            else:
                past_events.append(event_record)

        future_events.sort(key=lambda x: x[0])
        past_events.sort(key=lambda x: x[0], reverse=True)

        future_str = "\n".join(f"- {doc} (æ—¶é—´: {dt.astimezone(self.tz).strftime('%Y-%m-%d %H:%M')})" for dt, doc in future_events) or "æš‚æ— "
        past_str = "\n".join(f"- {doc} (æ—¶é—´: {dt.astimezone(self.tz).strftime('%Y-%m-%d %H:%M')})" for dt, doc in past_events[:5]) or "æš‚æ— "

        return future_str, past_str

    def get_all_event_memory_for_display(self, user_id: str) -> str:
        results = self.event_memory_collection.get(where={"user_id": user_id})
        if not results['ids']: return "æš‚æ— äº‹ä»¶è®°å¿†ã€‚"
        
        events_with_time = []
        aware_min_dt = datetime.datetime.min.replace(tzinfo=pytz.utc)

        for i, meta in enumerate(results['metadatas']):
            event_time_iso = meta.get('event_time_iso')
            doc = results['documents'][i]
            dt = aware_min_dt
            
            if event_time_iso:
                try:
                    parsed_dt = datetime.datetime.fromisoformat(event_time_iso.replace('Z', '+00:00'))
                    if parsed_dt.tzinfo is None:
                        dt = self.tz.localize(parsed_dt)
                    else:
                        dt = parsed_dt
                except (ValueError, TypeError):
                    doc = f"{doc} (æ—¶é—´è§£æå¤±è´¥: {event_time_iso})"
            
            events_with_time.append((dt, doc))
        
        events_with_time.sort(key=lambda x: x[0], reverse=True)
        
        formatted_events = []
        for dt, doc in events_with_time:
            if dt == aware_min_dt:
                formatted_events.append(f"- {doc}")
            else:
                local_dt = dt.astimezone(self.tz)
                formatted_events.append(f"- {doc} (æ—¶é—´: {local_dt.strftime('%Y-%m-%d %H:%M')})")

        return "\n".join(formatted_events) or "æš‚æ— äº‹ä»¶è®°å¿†ã€‚"

    def get_all_events_for_llm(self, user_id: str) -> list[dict]:
        results = self.event_memory_collection.get(where={"user_id": user_id})
        if not results['ids']: return []
        
        events = []
        for i, meta in enumerate(results['metadatas']):
            if 'event_id' in meta and 'event_time_iso' in meta:
                events.append({
                    "event_id": meta['event_id'],
                    "description": results['documents'][i],
                    "event_time_iso": meta['event_time_iso']
                })
        return events

    def clear_user_rag_documents(self, user_id: str):
        if self.rag_collection.get(where={"user_id": user_id})['ids']:
            self.rag_collection.delete(where={"user_id": user_id})
            logging.info(f"å·²æ¸…ç©ºç”¨æˆ· {user_id} çš„çŸ¥è¯†åº“ã€‚")

    def get_rag_file_list(self, user_id: str) -> list[str]:
        results = self.rag_collection.get(where={"user_id": user_id})
        if not results['ids']: return []
        return sorted(list({meta['source'] for meta in results['metadatas'] if 'source' in meta}))

    @st.cache_data(show_spinner=False)
    def query_rag_documents(_self, user_id: str, query: str, top_k: int = 3) -> str:
        if not _self.rag_collection.get(where={"user_id": user_id}, limit=1)['ids']: return ""
        results = _self.rag_collection.query(query_texts=[query], where={"user_id": user_id}, n_results=top_k)
        retrieved_docs = results.get("documents", [[]])[0]
        if not retrieved_docs: return ""
        return f"ä»ä½ çš„çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š\n" + "\n".join(f"- {doc}" for doc in retrieved_docs)
        
    @st.cache_data(show_spinner=False)
    def query_recent_discussions(_self, user_id: str, query: str, top_k: int = 3) -> str:
        if not _self.chat_collection.get(where={"user_id": user_id}, limit=1)['ids']: return "è¯¥ç”¨æˆ·æ²¡æœ‰ä»»ä½•å†å²å¯¹è¯è®°å½•ã€‚"
        results = _self.chat_collection.query(query_texts=[query], where={"user_id": user_id}, n_results=top_k)
        retrieved_docs = results.get("documents", [[]])[0]
        if not retrieved_docs: return "åœ¨ä½ çš„å†å²è®°å½•ä¸­ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸å½“å‰é—®é¢˜ç›¸å…³çš„å†…å®¹ã€‚"
        return f"ä½ å›å¿†èµ·äº†ä»¥ä¸‹å¯èƒ½ç›¸å…³çš„å†å²å¯¹è¯å†…å®¹ï¼š\n" + "\n".join(f"- \"{doc}\"" for doc in retrieved_docs)

# --- æ™ºèƒ½ä»£ç†ç±» ---
class ChatAgent:
    def __init__(self, config: Config, db_manager: ChatHistoryDB, api_key: str, user_id: str):
        self.config, self.db_manager, self.api_key, self.user_id = config, db_manager, api_key, user_id
        self.tz = pytz.timezone(config.TIMEZONE)
        if not api_key: raise ValueError("å¿…é¡»æä¾› ZHIPU_API_KEYã€‚")
        self.client = OpenAI(api_key=api_key, base_url=self.config.ZHIPU_BASE_URL)
        self.refresh_agent_state()

    def _get_date_reference(self, now: datetime.datetime) -> str:
        """åˆ›å»ºæœªæ¥ä¸€å‘¨çš„æ—¥æœŸå‚è€ƒå­—ç¬¦ä¸²"""
        weekdays_zh = {"Monday": "æ˜ŸæœŸä¸€", "Tuesday": "æ˜ŸæœŸäºŒ", "Wednesday": "æ˜ŸæœŸä¸‰", 
                       "Thursday": "æ˜ŸæœŸå››", "Friday": "æ˜ŸæœŸäº”", "Saturday": "æ˜ŸæœŸå…­", "Sunday": "æ˜ŸæœŸæ—¥"}
        date_references = []
        for i in range(7):
            future_date = now + datetime.timedelta(days=i)
            day_name = ""
            if i == 0: day_name = " (ä»Šå¤©)"
            elif i == 1: day_name = " (æ˜å¤©)"
            elif i == 2: day_name = " (åå¤©)"
            
            weekday_en = future_date.strftime('%A')
            weekday_zh = weekdays_zh.get(weekday_en, weekday_en)
            
            date_references.append(
                f"  - {future_date.strftime('%Y-%m-%d')}{day_name}, {weekday_zh}"
            )
        return "\n".join(date_references)

    def refresh_agent_state(self, query: str = None):
        """åˆ·æ–°ä»£ç†çŠ¶æ€æ—¶ï¼Œæ³¨å…¥ç²¾ç¡®çš„æ—¥æœŸå‚è€ƒä¿¡æ¯"""
        self.fact_memory_str = self.db_manager.load_fact_memory(self.user_id)
        self.future_events_str, self.past_events_str = self.db_manager.load_event_memory(self.user_id, query=query)
        self.all_events_display_str = self.db_manager.get_all_event_memory_for_display(self.user_id)
        
        now_time = datetime.datetime.now(self.tz)
        current_time_str = now_time.strftime('%Y-%m-%d %H:%M:%S %Z')
        
        date_reference_str = self._get_date_reference(now_time)

        system_prompt = self.config.SYSTEM_PROMPT_TEMPLATE.format(
            user_id=self.user_id,
            current_time=current_time_str,
            date_reference=date_reference_str,
            long_term_memory=self.fact_memory_str,
            future_events=self.future_events_str,
            past_events=self.past_events_str
        )
        
        self.messages = self.db_manager.load_history_by_user(self.user_id)
        if not self.messages or self.messages[0]['role'] != 'system':
            self.messages.insert(0, {"role": "system", "content": system_prompt})
        else:
            self.messages[0]['content'] = system_prompt

    def run(self, user_input: str):
        """
        ã€V5.6 ä¿®æ”¹ã€‘: é‡æ„çŠ¶æ€ç®¡ç†ï¼Œç¡®ä¿åœ¨æ‰€æœ‰å†™å…¥æ“ä½œåæ‰åˆ·æ–°çŠ¶æ€ã€‚
        """
        # 1. åˆ·æ–°çŠ¶æ€ï¼Œä¸ºå½“å‰å¯¹è¯å‡†å¤‡ä¸Šä¸‹æ–‡
        self.refresh_agent_state(query=user_input)
        
        # 2. å‡†å¤‡å‘é€ç»™ LLM çš„æ¶ˆæ¯åˆ—è¡¨
        context_from_rag = self.db_manager.query_rag_documents(self.user_id, user_input)
        context_from_history = self.db_manager.query_recent_discussions(self.user_id, user_input)
        
        messages_for_llm = list(self.messages)
        if context_from_rag: messages_for_llm.append({"role": "system", "content": f"è¡¥å……ä¿¡æ¯(æ¥è‡ªä½ çš„çŸ¥è¯†åº“ï¼Œå¯ä»¥å‚è€ƒä¸€ä¸‹):\n{context_from_rag}"})
        if context_from_history: messages_for_llm.append({"role": "system", "content": f"è¡¥å……ä¿¡æ¯(æ¥è‡ªæˆ‘ä»¬çš„å†å²å¯¹è¯ï¼Œä¹Ÿè®¸èƒ½æ´¾ä¸Šç”¨åœº):\n{context_from_history}"})
        
        messages_for_llm.append({"role": "user", "content": user_input})
        
        use_thinking_model = st.session_state.get('use_thinking_model', False)
        model_to_use = self.config.THINKING_MODEL if use_thinking_model else self.config.LLM_MODEL
        logging.info(f"æ­£åœ¨ä½¿ç”¨æ¨¡å‹: {model_to_use}")

        # 3. è°ƒç”¨ LLM å¹¶æµå¼ç”Ÿæˆå“åº”
        full_response_content = ""
        try:
            stream = self.client.chat.completions.create(
                model=model_to_use, 
                messages=messages_for_llm,
                stream=True
            )
            for chunk in stream:
                content_chunk = chunk.choices[0].delta.content or ""
                full_response_content += content_chunk
                yield content_chunk
                
        except Exception as e:
            logging.error(f"è°ƒç”¨LLM APIå¤±è´¥: {e}")
            full_response_content = f"æŠ±æ­‰ï¼Œå‡ºé”™äº†: {e}"
            yield full_response_content
        
        # --- æµå¼è¾“å‡ºç»“æŸåï¼Œæ‰§è¡Œåç»­çš„å†™å…¥å’ŒçŠ¶æ€æ›´æ–° ---
        
        # 4. æ›´æ–°å†…å­˜ä¸­çš„å¯¹è¯å†å²
        user_message = {"role": "user", "content": user_input}
        assistant_message = {"role": "assistant", "content": full_response_content}
        self.messages.extend([user_message, assistant_message])
        
        # 5. å°†æ–°å¯¹è¯å†™å…¥æ•°æ®åº“
        self.db_manager.save_message(self.user_id, user_message)
        self.db_manager.save_message(self.user_id, assistant_message)
        
        # 6. æå–å¹¶å†™å…¥æ–°çš„è®°å¿†åˆ°æ•°æ®åº“
        self.extract_and_save_memory()
        
        # 7. åœ¨æ‰€æœ‰å†™å…¥æ“ä½œå®Œæˆåï¼Œæœ€åç»Ÿä¸€åˆ·æ–°ä¸€æ¬¡çŠ¶æ€ï¼Œç¡®ä¿ä¸‹ä¸€è½®å¯¹è¯çš„ä¸Šä¸‹æ–‡æ˜¯å®Œæ•´çš„
        self.refresh_agent_state()

    def extract_and_save_memory(self):
        """ã€V5.6 ä¿®æ”¹ã€‘: ç§»é™¤æ­¤å‡½æ•°ä¸­çš„çŠ¶æ€åˆ·æ–°ï¼Œäº¤ç”±ä¸»æµç¨‹ run() ç»Ÿä¸€ç®¡ç†"""
        conversation = [msg for msg in self.messages if msg['role'] in ['user', 'assistant']]
        if len(conversation) < 2: return False
        
        recent_conversation = conversation[-6:]
        full_chat_content = "\n".join([f"{m['role']}: {m['content']}" for m in recent_conversation])
        
        now_time = datetime.datetime.now(self.tz)
        current_time_iso = now_time.isoformat()

        existing_events = self.db_manager.get_all_events_for_llm(self.user_id)
        existing_events_str = json.dumps(existing_events, ensure_ascii=False, indent=2) if existing_events else "[]"

        date_reference_str = self._get_date_reference(now_time)
        memory_prompt = f"""
        ä½ ç°åœ¨æ˜¯ä¸€ä½ä¸ºç”¨æˆ· {self.user_id} æœåŠ¡çš„ã€é«˜æ•ˆçš„è®°å¿†ç®¡å®¶ã€‚ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯åˆ†ææœ€æ–°çš„å¯¹è¯ï¼Œå¹¶ç®¡ç†ç”¨æˆ·çš„ä¸ªäººä¿¡æ¯å’Œæ—¥ç¨‹äº‹ä»¶ã€‚

        # 1. å·²æœ‰ä¿¡æ¯å‚è€ƒ
        - **å½“å‰ç²¾ç¡®æ—¶é—´**: `{current_time_iso}`
        - **æœªæ¥ä¸€å‘¨æ—¥æœŸå‚è€ƒ (ç”¨äºç²¾ç¡®è§£ææ—¶é—´)**:
{date_reference_str}
        - **å·²è®°å½•çš„æ—¥ç¨‹äº‹ä»¶**: 
        ```json
        {existing_events_str}
        ```

        # 2. ä½ çš„ä»»åŠ¡
        è¯·ä»”ç»†é˜…è¯»ä¸‹é¢çš„æœ€æ–°å¯¹è¯ï¼Œå¹¶ä»¥ä¸€ä¸ªJSONå¯¹è±¡çš„æ ¼å¼ï¼Œæ€»ç»“å‡ºä½ éœ€è¦æ‰§è¡Œçš„æ“ä½œã€‚è¿™ä¸ªJSONå¯¹è±¡åº”åŒ…å«ä¸¤éƒ¨åˆ†ï¼š`permanent_facts` å’Œ `event_actions`ã€‚

        ## `permanent_facts` (ç”¨æˆ·çš„æ ¸å¿ƒäº‹å®)
        - æç‚¼å…³äºç”¨æˆ·çš„ã€å‡ ä¹ä¸ä¼šæ”¹å˜çš„æ ¸å¿ƒä¿¡æ¯ï¼ˆå¦‚å§“åã€èŒä¸šã€é•¿æœŸåå¥½ï¼‰ã€‚
        - æ ¼å¼ä¸ºé”®å€¼å¯¹ã€‚å¦‚æœæ²¡æœ‰ï¼Œåˆ™ä¸ºç©ºå¯¹è±¡ `{{}}`ã€‚

        ## `event_actions` (æ—¥ç¨‹äº‹ä»¶ç®¡ç†)
        - è¿™æ˜¯ä¸€ä¸ªæ“ä½œæŒ‡ä»¤çš„åˆ—è¡¨ï¼Œç”¨äºç®¡ç†æ—¥ç¨‹ã€‚
        - **åˆ†æå¯¹è¯**ï¼šåˆ¤æ–­å¯¹è¯æ˜¯åœ¨**åˆ›å»ºæ–°äº‹ä»¶**ã€**æ›´æ–°ç°æœ‰äº‹ä»¶**è¿˜æ˜¯**åˆ é™¤ç°æœ‰äº‹ä»¶**ã€‚
        - **å‚è€ƒå·²æœ‰äº‹ä»¶**ï¼šåˆ©ç”¨ä¸Šé¢æä¾›çš„â€œå·²è®°å½•çš„æ—¥ç¨‹äº‹ä»¶â€åˆ—è¡¨æ¥åˆ¤æ–­ä¸€ä¸ªäº‹ä»¶æ˜¯æ–°çš„è¿˜æ˜¯å·²å­˜åœ¨çš„ã€‚
        - **ç”Ÿæˆæ“ä½œæŒ‡ä»¤**ï¼šæ ¹æ®ä½ çš„åˆ¤æ–­ï¼Œç”Ÿæˆä¸€ä¸ªæˆ–å¤šä¸ªæ“ä½œæŒ‡ä»¤ã€‚æ¯ä¸ªæŒ‡ä»¤éƒ½æ˜¯ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å« `type`, `event_id`, å’Œ `data` (ä»…upsertéœ€è¦)ã€‚

        ### æ“ä½œæŒ‡ä»¤è¯¦è§£:
        
        1.  **åˆ›å»º/æ›´æ–°äº‹ä»¶ (`upsert`)**:
            - `type`: "upsert"
            - `event_id`: **(å…³é”®!)**
                - å¦‚æœæ˜¯**æ–°äº‹ä»¶**ï¼Œè¯·æ ¹æ®äº‹ä»¶æ ¸å¿ƒå†…å®¹ï¼ˆå¦‚â€œå…¬å¸ä¼šè®®â€ã€â€œç”Ÿæ—¥æ´¾å¯¹â€ï¼‰å’Œæ—¥æœŸåˆ›é€ ä¸€ä¸ªç®€çŸ­ã€å”¯ä¸€çš„è‹±æ–‡IDï¼Œä¾‹å¦‚ `evt_meeting_20250703`ã€‚
                - å¦‚æœæ˜¯**æ›´æ–°äº‹ä»¶**ï¼Œè¯·ä»â€œå·²è®°å½•çš„æ—¥ç¨‹äº‹ä»¶â€åˆ—è¡¨ä¸­æ‰¾åˆ°å¹¶ä½¿ç”¨**å®Œå…¨ç›¸åŒ**çš„ `event_id`ã€‚
            - `data`:
                - `description`: äº‹ä»¶çš„å®Œæ•´æè¿°ã€‚
                - `event_time_iso`: **å¿…é¡»**å°†å¯¹è¯ä¸­çš„æ—¶é—´ï¼ˆå¦‚â€œæ˜å¤©ä¸‹åˆ3ç‚¹â€ï¼‰**ä¸¥æ ¼å‚ç…§ä¸Šé¢æä¾›çš„æ—¥æœŸå‚è€ƒ**ï¼Œè§£æä¸ºæ ‡å‡†çš„ISO 8601æ ¼å¼ (`YYYY-MM-DDTHH:MM:SSÂ±HH:MM`)ã€‚

        2.  **åˆ é™¤äº‹ä»¶ (`delete`)**:
            - `type`: "delete"
            - `event_id`: **(å…³é”®!)** ä»â€œå·²è®°å½•çš„æ—¥ç¨‹äº‹ä»¶â€åˆ—è¡¨ä¸­æ‰¾åˆ°ç”¨æˆ·æƒ³è¦åˆ é™¤çš„äº‹ä»¶ï¼Œå¹¶ä½¿ç”¨å…¶ `event_id`ã€‚

        # 3. è¾“å‡ºæ ¼å¼è¦æ±‚
        - æœ€ç»ˆè¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€å¯è¢«è§£æçš„JSONå¯¹è±¡ã€‚
        - å¦‚æœæ²¡æœ‰æå–åˆ°ä»»ä½•ä¿¡æ¯ï¼Œåˆ™æ¯ä¸ªå­—æ®µå¯¹åº”çš„å€¼åº”ä¸ºç©ºå¯¹è±¡æˆ–ç©ºåˆ—è¡¨ã€‚
        - ç¤ºä¾‹: `{{"permanent_facts": {{"nickname": "å°æ˜"}}, "event_actions": [{{"type": "upsert", "event_id": "evt_go_to_mars_20250801", "data": {{"description": "ç”¨æˆ·è®¡åˆ’å»ç«æ˜Ÿ", "event_time_iso": "2025-08-01T09:00:00+08:00"}}}}, {{"type": "delete", "event_id": "evt_old_meeting_20250701"}}]}}`

        ---
        æœ€æ–°å¯¹è¯å†…å®¹:
        {full_chat_content}
        ---
        
        æå–çš„JSON:
        """
        
        try:
            response = self.client.chat.completions.create(
                model=self.config.LLM_MODEL,
                messages=[{"role": "user", "content": memory_prompt}],
                response_format={"type": "json_object"}
            )
            content = response.choices[0].message.content
            if content and (extracted_data := json.loads(content)):
                has_new_data = (extracted_data.get('permanent_facts') or 
                                extracted_data.get('event_actions'))
                if has_new_data:
                    self.db_manager.save_structured_memory(self.user_id, extracted_data)
                    logging.info(f"ç”¨æˆ· {self.user_id} çš„æ–°è®°å¿†å·²å¤„ç†ã€‚")
                    return True
            return False
        except Exception as e:
            logging.error(f"è§£ææˆ–ä¿å­˜é•¿æœŸè®°å¿†å¤±è´¥: {e}\nå“åº”å†…å®¹: {content if 'content' in locals() else 'N/A'}")
            return False

# -----------------------------------------------------------------------------
# æ­¥éª¤ 3: Streamlit å‰ç«¯ç•Œé¢
# -----------------------------------------------------------------------------

st.set_page_config(page_title="æ‚¨çš„ä¸“å±è®°å¿†ä¼™ä¼´", page_icon="ğŸ¤—", layout="centered")

@st.cache_resource
def get_core_services():
    config = Config()
    db_manager = ChatHistoryDB(config)
    return config, db_manager

config, db_manager = get_core_services()
api_key = os.getenv("ZHIPUAI_API_KEY")
if not api_key:
    st.error("æœªæ‰¾åˆ° ZHIPUAI_API_KEY ç¯å¢ƒå˜é‡ï¼Œè¯·é…ç½®ã€‚")
    st.stop()
    
if "logged_in_user_id" not in st.session_state: st.session_state.logged_in_user_id = None
if "agent" not in st.session_state: st.session_state.agent = None
if "last_uploaded_file_id" not in st.session_state: st.session_state.last_uploaded_file_id = None
if 'use_thinking_model' not in st.session_state: st.session_state.use_thinking_model = False


# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.header("ğŸ‘¤ ç”¨æˆ·ä¸­å¿ƒ")
    user_id_input = st.text_input("è¯·è¾“å…¥æ‚¨çš„ç”¨æˆ·ID", key="user_id_input", placeholder="ä¾‹å¦‚: zhangsan")
    if st.button("ç™»å½• / åˆ‡æ¢ç”¨æˆ·", key="login_button"):
        if user_id_input:
            if st.session_state.agent and st.session_state.logged_in_user_id != user_id_input:
                st.session_state.agent.extract_and_save_memory()
            
            st.session_state.logged_in_user_id = user_id_input
            
            with st.spinner(f"æ­£åœ¨ä¸ºæ‚¨æ¸…ç†è¿‡æœŸçš„ä¸´æ—¶è®°å¿†..."):
                db_manager.clear_old_temporal_fact_memory(user_id_input)
                time.sleep(0.5)

            with st.spinner(f"æ­£åœ¨ä¸ºæ‚¨æ¸…ç†è¿‡æœŸçš„æ—¥ç¨‹..."):
                db_manager.clear_expired_events(user_id_input)
                time.sleep(0.5)

            st.session_state.agent = None
            st.toast(f"æ¬¢è¿å›æ¥, {user_id_input}ï¼å¾ˆé«˜å…´å†æ¬¡è§åˆ°ä½ ã€‚", icon="ğŸ¤—")
            st.rerun()
        else: st.warning("è¯·è¾“å…¥ä¸€ä¸ªç”¨æˆ·IDã€‚")

    if st.session_state.logged_in_user_id:
        current_user_id = st.session_state.logged_in_user_id
        st.markdown("---")
        
        st.header("âš™ï¸ æ¨¡å‹è®¾ç½®")
        use_thinking_model_toggle = st.toggle(
            f"å¯ç”¨é«˜çº§æ€è€ƒæ¨¡å‹ ({config.THINKING_MODEL})", 
            key='use_thinking_model', 
            help=f"å¼€å¯åï¼Œå¯¹è¯å°†ä½¿ç”¨æ›´å¼ºå¤§çš„ {config.THINKING_MODEL} æ¨¡å‹ã€‚é»˜è®¤ä½¿ç”¨ {config.LLM_MODEL}ã€‚"
        )

        st.markdown("---")
        
        st.header("ğŸ“š çŸ¥è¯†åº“ (RAG)")
        uploaded_file = st.file_uploader("åˆ†äº«ä¸€äº›èµ„æ–™ç»™æˆ‘å­¦ä¹  (.txt/.md)", type=['txt', 'md'], key=f"uploader_{current_user_id}")
        
        if uploaded_file is not None and uploaded_file.file_id != st.session_state.get('last_uploaded_file_id'):
            progress_container = st.empty()
            try:
                def update_progress(percent, message):
                    progress_container.progress(percent, text=message)
                
                content = uploaded_file.getvalue().decode("utf-8")
                db_manager.add_document_to_rag(current_user_id, uploaded_file.name, content, progress_callback=update_progress)
                
                st.session_state.last_uploaded_file_id = uploaded_file.file_id
                
                time.sleep(1)
                progress_container.empty()
                st.toast(f"è°¢è°¢ä½ çš„åˆ†äº«ï¼Œ'{uploaded_file.name}' æˆ‘å·²ç»å­¦ä¹ å®Œå•¦ï¼", icon="âœ…")
                st.rerun()
            except Exception as e:
                st.session_state.last_uploaded_file_id = uploaded_file.file_id
                progress_container.empty()
                st.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
        
        rag_files = db_manager.get_rag_file_list(current_user_id)
        if rag_files:
            with st.expander("æŸ¥çœ‹æˆ‘å­¦ä¹ è¿‡çš„èµ„æ–™", expanded=False):
                for f in rag_files: st.caption(f)
            if st.button("ğŸ—‘ï¸ å¿˜è®°æ‰€æœ‰å­¦ä¹ èµ„æ–™", key="clear_rag"):
                with st.spinner("æ­£åœ¨æ¸…ç©ºæˆ‘çš„å­¦ä¹ ç¬”è®°..."):
                    db_manager.clear_user_rag_documents(current_user_id)
                    st.cache_data.clear()
                st.toast("æˆ‘å·²ç»å¿˜è®°æ‰€æœ‰å­¦ä¹ è¿‡çš„èµ„æ–™å•¦ã€‚", icon="ğŸ—‘ï¸")
                st.rerun()

        st.markdown("---")
        st.header("ğŸ› ï¸ è®°å¿†å·¥å…·ç®±")
        
        if st.button("ğŸ”„ æ¸…ç†åº”ç”¨ç¼“å­˜", key="clear_app_cache", help="å½“åº”ç”¨è¡Œä¸ºå¼‚å¸¸æˆ–ä»£ç æ›´æ–°åæœªç”Ÿæ•ˆæ—¶ï¼Œå¯å°è¯•æ¸…ç†ç¼“å­˜ã€‚"):
            st.cache_resource.clear()
            st.cache_data.clear()
            st.session_state.agent = None
            st.toast("åº”ç”¨ç¼“å­˜å·²æ¸…ç†ï¼åº”ç”¨å°†é‡æ–°åŠ è½½ã€‚", icon="â™»ï¸")
            time.sleep(1)
            st.rerun()

        if st.button("ğŸ§  æ‰‹åŠ¨å›é¡¾ä¸€ä¸‹", key="extract_memory", help="é€šå¸¸æˆ‘ä¼šè‡ªåŠ¨è®°å¿†ï¼Œè¿™ä¸ªæŒ‰é’®å¯ä»¥è®©æˆ‘ç«‹å³å¼ºåˆ¶å›é¡¾æˆ‘ä»¬çš„å¯¹è¯ã€‚"):
            if st.session_state.agent:
                with st.spinner("æ­£åœ¨å¼ºåˆ¶å›é¡¾å¯¹è¯..."):
                    saved = st.session_state.agent.extract_and_save_memory()
                    # åœ¨æ‰‹åŠ¨å›é¡¾åä¹Ÿåˆ·æ–°ä¸€æ¬¡ä¸»çŠ¶æ€
                    st.session_state.agent.refresh_agent_state()
                if saved:
                    st.success("âœ… å›é¡¾å®Œæˆï¼Œåˆæœ‰æ–°æ”¶è·ï¼")
                else:
                    st.info("ğŸ¤·â€ æš‚æ—¶æ²¡æœ‰å‘ç°æ–°çš„ä¿¡æ¯å¯ä»¥è®°å½•ã€‚")
                time.sleep(2)
                st.rerun()

        if st.button("ğŸ§¹ å¼€å§‹ä¸€æ®µæ–°å¯¹è¯", key="clear_chat"):
            if st.session_state.agent:
                st.session_state.agent.extract_and_save_memory()
                with st.spinner("æ­£åœ¨æ¸…ç©ºæˆ‘ä»¬çš„å¯¹è¯..."):
                    db_manager.clear_user_chat_history(current_user_id)
                    st.cache_data.clear()
                st.session_state.agent = None
                st.toast("å¥½äº†ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹æ–°çš„è¯é¢˜äº†ï¼", icon="ğŸ’¬")
                st.rerun()
        
        with st.expander("ğŸ‘€ çœ‹çœ‹å…³äºä½ çš„è®°å¿†"):
            if 'agent' in st.session_state and st.session_state.agent:
                memory_content = st.session_state.agent.fact_memory_str
                if memory_content.strip() and memory_content != "æš‚æ— ":
                    st.code(memory_content, language=None)
                else:
                    st.info("å…³äºä½ çš„äº‹ï¼Œæˆ‘è¿˜äº†è§£å¾—ä¸å¤šã€‚")

        with st.expander("ğŸ“… çœ‹çœ‹æˆ‘ä»¬çš„æ—¥ç¨‹å’Œäº‹ä»¶"):
            if 'agent' in st.session_state and st.session_state.agent:
                event_memory_content = st.session_state.agent.all_events_display_str
                if event_memory_content.strip() and "æš‚æ— " not in event_memory_content:
                    st.code(event_memory_content, language=None)
                else:
                    st.info("æˆ‘ä»¬ä¹‹é—´è¿˜æ²¡æœ‰å‘ç”Ÿä»€ä¹ˆç‰¹åˆ«çš„äº‹ã€‚")

    else: st.caption("è¯·å…ˆç™»å½•ï¼Œè®©æˆ‘è®¤è¯†ä½ ã€‚")

# --- ä¸»èŠå¤©ç•Œé¢ ---
st.title("ğŸ¤— æ‚¨çš„ä¸“å±è®°å¿†ä¼™ä¼´ V5.6")
st.caption("æˆ‘åœ¨è¿™é‡Œï¼Œéšæ—¶å‡†å¤‡å€¾å¬ã€æ”¯æŒå’Œé™ªä¼´ã€‚")
if not st.session_state.logged_in_user_id:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾¹æ è¾“å…¥ä½ çš„IDï¼Œè®©æˆ‘è®¤è¯†ä½ å§ã€‚")
    st.stop()
try:
    if st.session_state.agent is None:
        st.session_state.agent = ChatAgent(config, db_manager, api_key, st.session_state.logged_in_user_id)
except Exception as e:
    st.error(f"åˆå§‹åŒ–ä¼™ä¼´æ—¶å‡ºé”™äº†: {e}")
    st.stop()

for message in st.session_state.agent.messages:
    if message["role"] in ["user", "assistant"]:
        with st.chat_message(message["role"]):
            content_to_display = message["content"]
            if message["role"] == "assistant":
                content_to_display = process_response_for_display(content_to_display)
            
            if content_to_display:
                st.markdown(content_to_display)

if prompt := st.chat_input(f"å—¨, {st.session_state.logged_in_user_id}, åœ¨æƒ³äº›ä»€ä¹ˆå‘¢?"):
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_response = ""
        for chunk in st.session_state.agent.run(prompt):
            full_response += chunk
            placeholder.markdown(process_response_for_display(full_response) + "â–Œ")
        placeholder.markdown(process_response_for_display(full_response))
